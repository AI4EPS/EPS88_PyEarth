{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJsUjs19_v63"
   },
   "source": [
    "<!-- # MNIST with SciKit-Learn and PyTroch -->\n",
    "# Neural Networks with PyTorch\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4EPS/EPS88_PyEarth/blob/master/docs/lectures/10_neural_networks2.ipynb\">\n",
    "<img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zmIlvxI_v68"
   },
   "source": [
    "**Note**: If you are running this in [a colab notebook](https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb), we recommend you enable a free GPU by going:\n",
    "\n",
    "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks shows how to define and train a simple modern Neural Network with PyTorch.\n",
    "\n",
    "We will use the MNIST dataset, which is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n",
    "\n",
    "The dateaset is very popular and is often used as a \"Hello World\" example in the field of Machine Learning. It is a good dataset to start with, as it is small and easy to work with.\n",
    "\n",
    "For this small dataset, we will use the socalled \"LeNet\" architecture is used here. It is a simple convolutional neural network, which was introduced by Yann LeCun in 1998. It is a simple and effective architecture for small image datasets. \n",
    "You can read more about the model on [Wikipedia](https://en.wikipedia.org/wiki/LeNet).\n",
    "\n",
    "<!-- ![](https://raw.githubusercontent.com/zhuwq0/images/main/lenet.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/zhuwq0/images/main/lenet.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj0pvjxT_v7G"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPz6Bjqw_v7H"
   },
   "source": [
    "## Loading Data\n",
    "Using SciKit-Learns ```fetch_openml``` to load MNIST data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image of the MNIST dataset is encoded in a 784 dimensional vector, representing a 28 x 28 pixel image.\n",
    "\n",
    "Each pixel has a value between 0 and 255, corresponding to the grey-value of a pixel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwpfASvc_v7J"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Pt2JKyb_v7K",
    "outputId": "5a96aa80-e889-4553-c289-9534ed68d708"
   },
   "outputs": [],
   "source": [
    "print(f\"MNIST data shape: {mnist.data.shape}, data type: {mnist.data.dtype}\")\n",
    "print(f\"MNIST target shape: {mnist.target.shape}, target type: {mnist.target.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV0ehb52_v7L"
   },
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "The above ```featch_mldata``` method to load MNIST returns ```data``` and ```target``` as ```uint8``` which we convert to ```float32``` and ```int64``` respectively.\n",
    "\n",
    "To avoid big weights that deal with the pixel values from between [0, 255], we scale `X` down. A commonly used range is [0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2v_Fwne_v7M"
   },
   "outputs": [],
   "source": [
    "## Make sure the data is float32 and the labels are int64\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "## Normalize the data to [0, 1]. Hint: the raw pixel values are in [0, 255].\n",
    "X =\n",
    "print(f\"{X.min() = }, {X.max() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as prevoious lectures, let split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gILlsHJS_v7R"
   },
   "outputs": [],
   "source": [
    "## Split data into training and testing sets using 30% of the data for testing and random seed 42\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EKEvbuP_v7c"
   },
   "source": [
    "### Visualize a selection of training images and their labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9muXJPC_v7d"
   },
   "outputs": [],
   "source": [
    "## Define a function to plot a selection of images and their labels\n",
    "def plot_example(X, y, n_samples=10):\n",
    "    \"\"\"Plot the first n_samples images and their labels in a row.\"\"\"\n",
    "    fig, axes = plt.subplots(1, n_samples, figsize=(2*n_samples, 4))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(y[i], fontsize=32)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "h2-R1-Df_v7e",
    "outputId": "619a14f4-7a23-4a09-a872-e646cf5c5900"
   },
   "outputs": [],
   "source": [
    "## Plot the first 10 images from the training set\n",
    "plot_example("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQvC-rWf_v7f"
   },
   "source": [
    "## Build Neural Network with PyTorch\n",
    "\n",
    "In the previous lecture, we have built a simple fully connected neural network with one hidden layer for both linear regression and classification tasks. Let's first try a similar network for the classification task of MNIST.\n",
    "\n",
    "Note the dataset is much larger than our previous examples, so we need to adjust the network size accordingly.\n",
    "(It is still tiny compared to modern standards)\n",
    "\n",
    "Let's think about the network architecture:\n",
    "\n",
    "- Input layer: 784 dimensions (28x28). This is defined by the MNIST data shape.\n",
    "- Hidden layer: 98 (= 784 / 8). This is a free parameter that we can choose.\n",
    "- Output layer: 10 neurons, representing digits 0 - 9. This is defined by the number of classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load PyTorch for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PG7R0W8_v7f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeVnFhBS_v7i"
   },
   "source": [
    "- Build a simple fully connected neural network in PyTorch's framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the network architecture: A simple fully connected neural network\n",
    "class FCN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=28*28,\n",
    "            hidden_dim=28*4,\n",
    "            output_dim=10,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(FCN, self).__init__()\n",
    "        ## Define the neural network layers\n",
    "        self.fc1 = \n",
    "        self.fc2 = \n",
    "        self.dropout = \n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        ## Apply the neural network layers\n",
    "        x = \n",
    "        x = \n",
    "        x = \n",
    "        x = \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model dimensions\n",
    "mnist_dim = X.shape[1]\n",
    "hidden_dim = int(mnist_dim/8)\n",
    "output_dim = len(np.unique(mnist.target))\n",
    "\n",
    "## Define the model, loss function, and optimizer\n",
    "model = \n",
    "criterion = \n",
    "optimizer = \n",
    "\n",
    "## Define the device to choose the fastest for training\n",
    "## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "## Define fit function\n",
    "def fit(model, X_train, y_train, epochs=100):\n",
    "    dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)\n",
    "    model.to(device)\n",
    "\n",
    "    ## set the model to training mode\n",
    "    model.\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):\n",
    "            X_train = X_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            ## zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## get the model predictions\n",
    "            outputs = \n",
    "            ## calculate the loss\n",
    "            batch_loss = \n",
    "            batch_loss.backward()\n",
    "            ## update the weights\n",
    "            optimizer.\n",
    "            \n",
    "            loss += batch_loss.item()\n",
    "\n",
    "        # average loss per batch\n",
    "        loss = loss / len(dataloader)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')\n",
    "\n",
    "    return losses\n",
    "\n",
    "## Define predict function\n",
    "def predict(model, X):\n",
    "    dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)\n",
    "    ## set the model to evaluation mode\n",
    "    model.\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        predicted = []\n",
    "        for X, in tqdm(dataloader, desc='Predicting'):\n",
    "            X = X.to(device)\n",
    "            \n",
    "            ## get the model predictions\n",
    "            outputs =\n",
    "\n",
    "            _, predicted_batch = torch.max(outputs, 1)\n",
    "            predicted.append(predicted_batch.cpu())\n",
    "    return torch.cat(predicted)\n",
    "\n",
    "## Train the model\n",
    "losses = fit(model, X_train_tensor, y_train_tensor, epochs=15)\n",
    "\n",
    "## Plot the loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the model on the test set. This is same as the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on the test set\n",
    "y_pred_tensor = \n",
    "y_pred = y_pred_tensor.numpy()\n",
    "\n",
    "## Calculate accuracy\n",
    "accuracy = \n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "## Confusion matrix\n",
    "conf_matrix = \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y))))\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eRga6AV_v7o"
   },
   "source": [
    "What accuracy did you get? Is it above 95%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of above 95% for a network with only one hidden layer is not too bad.\n",
    "\n",
    "Let's take a look at some predictions that went wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XddXR1_a_v7p"
   },
   "outputs": [],
   "source": [
    "error_mask_fcn = y_pred != y_test\n",
    "plot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn], n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these errors reasonable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2GsBaxH_v7r"
   },
   "source": [
    "# Convolutional Network\n",
    "\n",
    "To further improve the performance, let's try a convolutional neural network (CNN) for MNIST. \n",
    "\n",
    "The 2D convolutional layer expects a 4 dimensional tensor as input. The dimensions represent:\n",
    "* Batch size\n",
    "* Number of channel\n",
    "* Height\n",
    "* Width\n",
    "\n",
    "So we need to reshape the MNIST data to have the right shape.\n",
    "MNIST data has only one channel. As stated above, each MNIST vector represents a 28x28 pixel image. Hence, the resulting shape for PyTorch tensor needs to be (x, 1, 28, 28). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert data to PyTorch tensors and reshape to 4D tensor (batch_size, channel, height, width)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).reshape(-1, 1, 28, 28)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).reshape(-1, 1, 28, 28)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build a simple convolutional neural network in PyTorch's framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdQ-ISvb_v7u"
   },
   "outputs": [],
   "source": [
    "## Define the network architecture: A simple convolutional neural network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim=28*28, hidden_dim=112, output_dim=10, dropout=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        ## Define the neural network layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv1_drop = nn.Dropout2d(p=dropout)\n",
    "        self.conv2_drop = nn.Dropout2d(p=dropout)\n",
    "        num_features = 64 * int((input_dim**0.5 // 4 - 2)**2)\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim) # 1600 = number channels * width * height\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc1_drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ## convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv1_drop(x)\n",
    "\n",
    "        ## convolutional layers\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2_drop(x)\n",
    "        \n",
    "        ## flatten over channel, height and width        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        ## fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1_drop(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model dimensions\n",
    "mnist_dim = X.shape[1]\n",
    "hidden_dim = int(mnist_dim/8)\n",
    "output_dim = len(np.unique(mnist.target))\n",
    "\n",
    "## Define the model, loss function, and optimizer\n",
    "model = \n",
    "criterion = \n",
    "optimizer =\n",
    "\n",
    "## Define the device to choose the fastest for training\n",
    "## MPS for Apple Silicon, CUDA for NVidia GPUs, and CPU otherwise\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "## Define fit function\n",
    "def fit(model, X_train, y_train, epochs=100):\n",
    "\n",
    "    dataloader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=128, shuffle=True, drop_last=True)\n",
    "    model.to(device)\n",
    "\n",
    "    ## set the model to training mode\n",
    "    model.\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for X_train, y_train in tqdm(dataloader, desc=f'Training Epoch {epoch+1}/{epochs}'):\n",
    "            X_train = X_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            ## zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## get the model predictions\n",
    "            outputs = \n",
    "            ## calculate the loss\n",
    "            batch_loss = \n",
    "            batch_loss.backward()\n",
    "            ## update the weights\n",
    "            optimizer.\n",
    "\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "        # average loss per batch\n",
    "        loss = loss / len(dataloader)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}: Loss: {loss:.4f}')\n",
    "\n",
    "    return losses\n",
    "\n",
    "## Define predict function\n",
    "def predict(model, X):\n",
    "\n",
    "    dataloader = DataLoader(dataset=TensorDataset(X), batch_size=128, drop_last=False)\n",
    "\n",
    "    ## set the model to evaluation mode\n",
    "    model.\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        predicted = []\n",
    "        for X, in tqdm(dataloader, desc='Predicting'):\n",
    "            X = X.to(device)\n",
    "\n",
    "            ## get the model predictions\n",
    "            outputs = \n",
    "            \n",
    "            _, predicted_batch = torch.max(outputs, 1)\n",
    "            predicted.append(predicted_batch.cpu())\n",
    "    return torch.cat(predicted)\n",
    "\n",
    "## Train the model\n",
    "losses = fit(model, X_train_tensor, y_train_tensor, epochs=15)\n",
    "\n",
    "## Plot the loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on the test set\n",
    "y_pred_tensor = \n",
    "y_pred = y_pred_tensor.numpy()\n",
    "\n",
    "## Calculate accuracy\n",
    "accuracy = \n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "## Confusion matrix\n",
    "conf_matrix = \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(len(np.unique(y))))\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What accuracy did you get? Is it better than the fully connected network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of >98% should suffice for this example!\n",
    "\n",
    "Let's take a look at some predictions that went wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mask_cnn = y_pred != y_test\n",
    "plot_example(X_test[error_mask_cnn], y_pred[error_mask_cnn], n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npZlem33_v7v"
   },
   "source": [
    "- Let's further look at the accuracy of the convolutional network model (CNN) on the misclassified examples previously by the fully connected network (FCN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oWL0xGC_v7w",
    "outputId": "449de735-c1a9-4301-d758-e07ac678d18f"
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test[error_mask_fcn], y_pred[error_mask_fcn])\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8239U9fF_v7w"
   },
   "source": [
    "About 70% of the previously misclassified images are now correctly identified. \n",
    "\n",
    "Let's take a look at some of the misclassified examples before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "5vU1SXeV_v7x",
    "outputId": "332726a3-6c15-49c2-860d-b48eee5c85e2"
   },
   "outputs": [],
   "source": [
    "plot_example(X_test[error_mask_fcn], y_pred[error_mask_fcn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Last questions. Please take a look at the training loops of the fully connected network and the convolutional network. Comment on the similarities and differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see although the network architecture is different, the training loops are very similar. \n",
    "\n",
    "This is one feature of neural networks models. You can build significantly larger models and train them efficiently with a similar training loop, as long as you have enough computational power.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus points: One challenge in deep learning training is to find the proper hyperparameters. There are many hyperparameters to tune, such as the learning rate, the number of hidden layers, the number of neurons in each layer, the batch size, the number of epochs, etc.\n",
    "\n",
    "Try to tune the hyperparameters of the convolutional network model to achieve a higher accuracy. For the MNIST dataset, an accuracy of >99% is possible with the LeNet architecture. Could you achieve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd97b8bffa4d3737e84826bc3d37be3046061822757ce35137ab82ad4c5a2016"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
