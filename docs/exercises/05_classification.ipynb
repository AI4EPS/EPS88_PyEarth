{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning: classification of basalt source\n",
    "\n",
    "## Import scientific python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "Text from: https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), it is said to have several attributes or features.\n",
    "\n",
    "Learning problems fall into a few categories:\n",
    "- **supervised learning**, in which the data comes with additional attributes that we want to predict (https://scikit-learn.org/stable/supervised_learning.html). This problem can be either:\n",
    "    - *regression*: if the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight.\n",
    "    - *classification*: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. An example of a classification problem would be handwritten digit recognition, in which the aim is to assign each input vector to one of a finite number of discrete categories. Another way to think of classification is as a discrete (as opposed to continuous) form of supervised learning where one has a limited number of categories and for each of the n samples provided, one is to try to label them with the correct category or class.\n",
    "\n",
    "- **unsupervised learning**, in which the training data consists of a set of input vectors x without any corresponding target values. The goal in such problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization (https://scikit-learn.org/stable/unsupervised_learning.html).\n",
    "\n",
    "### Training set and testing set\n",
    "\n",
    "Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the training set, on which we learn some properties; we call the other set the testing set, on which we test the learned properties.\n",
    "\n",
    "**Today we will focus on classification through a supervised learning approach**\n",
    "\n",
    "*Systems doing this type of analysis are all around us. Consider a spam filter for example*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying volcanic rocks\n",
    "\n",
    "<img src=\"./images/volcanic-tectonics.png\" width = 600 align = 'center'>\n",
    "\n",
    "Today we are going to deal with igneous geochemistry data. Igneous rocks are those that crystallize from cooling magma. Different magmas have different compositions associated with their origin as we explored a week ago. During class today, we will focus on data from mafic lava flows (these are called basalts and are the relatively low silica, high iron end of what we looked at last week).\n",
    "\n",
    "> Igneous rocks form in a wide variety of tectonic settings,\n",
    "including mid-ocean ridges, ocean islands, and volcanic\n",
    "arcs. It is a problem of great interest to igneous petrologists\n",
    "to recover the original tectonic setting of mafic rocks of the\n",
    "past. When the geological setting alone cannot unambiguously\n",
    "resolve this question, the chemical composition of\n",
    "these rocks might contain the answer. The major, minor,\n",
    "and trace elemental composition of basalts shows large\n",
    "variations, for example as a function of formation depth\n",
    "(e.g., Kushiro and Kuno, 1963) --- *Vermeesch (2006)*\n",
    "\n",
    "For this analysis, we are going to use a dataset that was compiled in \n",
    "\n",
    "Vermeesch (2006) Tectonic discrimination of basalts with classification trees, *Geochimica et Cosmochimica Acta*  https://doi.org/10.1016/j.gca.2005.12.016\n",
    "\n",
    "These data were grouped into 3 categories:\n",
    "\n",
    "- 256 ***Island arc basalts (IAB)*** from the Aeolian, Izu-Bonin, Kermadec, Kurile, Lesser Antilles, Mariana, Scotia, and Tonga arcs.\n",
    "- 241 ***Mid-ocean ridge (MORB)*** samples from the East Pacific Rise, Mid Atlantic Ridge, Indian Ocean, and Juan de Fuca Ridge.\n",
    "- 259 ***Ocean-island (OIB)*** samples from St. Helena, the Canary, Cape Verde, Caroline, Crozet, Hawaii-Emperor, Juan Fernandez, Marquesas, Mascarene, Samoan, and Society islands.\n",
    "\n",
    "Let's look at the illustration above and determine where each of these settings are within a plate tectonic context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "\n",
    "The data are from the supplemental materials of the Vermeesch (2006) paper. The samples are grouped by affinity MORB, OIB, and IAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the file data/Vermeesch2006.csv of the basalt data into a pandas DataFrame\n",
    "basalt_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a look at the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can geochemical data be used to classify the tectonic setting?\n",
    "\n",
    "These data are labeled. The author already determined what setting these basalts came from. However, is there are way that we could use these labeled data to determine the setting for an unknown basalt?\n",
    "\n",
    "A paper published in 1982 proposed that the elements titanium and vanadium were particular good at giving insight into tectonic setting. The details of why are quite complicated and can be summarized as \"the depletion of V relative to Ti is a function of the fO2 of the magma and its source, the degree of partial melting, and subsequent fractional crystallization.\" If you take EPS100B you will learn more about the fundamentals behind this igneous petrology. *For the moment, you can consider the working hypothesis behind this classification to that different magmatic environments have differences in oxidation states that are reflected in Ti vs V ratios.*\n",
    "\n",
    "Shervais, J.W. (1982) Ti-V plots and the petrogenesis of modern and ophiolitic lavas *Earth and Planetary Science Letters* https://doi.org/10.1016/0012-821X(82)90120-0\n",
    "\n",
    "### Plot TiO2 (wt%) vs V (ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "## Loop through the unique values of the 'affinity' column to plot each group\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the pandas groupby function to group by affinity and describe the values of column 'TiO2_wt_percent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Use the groupby and describe methods to get a summary of the TiO2 content for each affinity group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Try the other column of 'V_ppm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the groupby and describe methods to get a summary of the V content for each affinity group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Can you differentiate between the different affinities on titanium or vanadium concentration alone?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye test classification method\n",
    "\n",
    "In order to classify the basalt into their affinity based on titanium and vanadium concentrations, we can use a classification method.\n",
    "\n",
    "The goal here is to be able to make an inference of what environment an unknown basalt formed in based on comparison to these data.\n",
    "\n",
    "Let's say that we have three points where their affinity is unknown.\n",
    "- point 1 has TiO2 of 4% and V concentration of 300 ppm\n",
    "- point 2 has TiO2 of 1% and V concentration of 350 ppm\n",
    "- point 3 has TiO2 of 1.9% and V concentration of 200 ppm\n",
    "\n",
    "What do you think the classification of these three points should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data on the TiO2 vs V plot and see if we can visually classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = {'TiO2_wt_percent': 4, 'V_ppm': 300}\n",
    "sample2 = {'TiO2_wt_percent': 1, 'V_ppm': 350}\n",
    "sample3 = {'TiO2_wt_percent': 1.6, 'V_ppm': 280}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for affinity in basalt_data['affinity'].unique():\n",
    "    subset = basalt_data[basalt_data['affinity'] == affinity]\n",
    "    plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)\n",
    "\n",
    "## Add the unknown samples to the plot using different markers and labels\n",
    "\n",
    "\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A linear classification\n",
    "\n",
    "An approach that has been taken in volcanic geochemistry is to draw lines to use for classification.\n",
    "\n",
    "We are using the package scikit-learn in order to implement such a classification. Scikit-learn is a widely-used Python library for machine learning and data analysis. It provides a wide range of tools for data preprocessing, model selection, and evaluation. Its user-friendly interface and extensive documentation make it a popular choice for researchers, data analysts, and machine learning practitioners.\n",
    "\n",
    "![scikit-learn logo](https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import sci-kit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our classifier\n",
    "\n",
    "We can use `LogisticRegression` from scikit-learn as a classifier. The algorithm finds the best straight line, also called a hyperplane, to separate different groups of data. \n",
    "\n",
    "Once the lines have been found they can be used predict the group of new data points based on which side of the line they fall on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the classifier using the LogisticRegression class from scikit-learn. Hint: use the 'liblinear' solver\n",
    "classifier_linear = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for classification \n",
    "\n",
    "We need to do a bit of prep work on the data first as not all of the data have Ti (wt %) and V (ppm) data. \n",
    "\n",
    "Let's define a new dataframe `basalt_data_Ti_V` that has the rows that contain both values. This will result in us using fewer data than is in the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a new DataFrame with only the samples that have both TiO2 and V data available. Hint: use the .notna() method\n",
    "basalt_data_Ti_V = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the number of samples in the original DataFrame basalt_data and the new DataFrame basalt_data_Ti_V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning literature and code conventions, uppercase \"X\" is often used to represent the matrix of feature variables (predictors), while lowercase \"y\" is used to represent the target variable (response).\n",
    "\n",
    "This notation is used to visually distinguish between the two variables and indicate that \"X\" is a matrix (usually with multiple columns for features), while \"y\" is a vector (usually with a single column representing the target variable). \n",
    "\n",
    "The uppercase \"X\" signifies a multi-dimensional data structure, and the lowercase \"y\" signifies a one-dimensional data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's define X as the features of ['TiO2_wt_percent', 'V_ppm'] and y as the target variable 'affinity'\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables that represent different categories that we have here are: 'MORB', 'IAB', and 'OIB'. However, most machine learning algorithms require numerical inputs. \n",
    "\n",
    "Label encoding is a technique that transforms the categorical variables into numerical labels. We can use the `sklearn.preprocessing` `LabelEncoder` function to do this task for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## let's encode the target variable y using the LabelEncoder class from scikit-learn\n",
    "le = LabelEncoder()\n",
    "y_encoded = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the original 'affinity' categories and the encoded categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the original and encoded affinity values. Hint: use the np.unique() function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit/train the classifier\n",
    "\n",
    "Now that we have `X` as DataFrame of `['TiO2_wt_percent', 'V_ppm']` and `y_encoded` as numerical representation of the categories we can fit the classifier to the data.\n",
    "\n",
    "To do this, we feed the DataFrame of the data and the array of the classification into a `.fit` function preformed on the classifier object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the classifier using the fit method with X and y_encoded as arguments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision boundaries\n",
    "\n",
    "Let's make a 101 x 101 grid of x and y values between 0 and the maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of points over the feature space\n",
    "xx, yy = np.meshgrid(np.linspace(0, max(basalt_data_Ti_V['TiO2_wt_percent']), 101),\n",
    "                     np.linspace(0, max(basalt_data_Ti_V['V_ppm']), 101))\n",
    "grid = np.array([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(xx, yy, s=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the grid\n",
    "\n",
    "We can then predict the class labels for each point in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_classes = classifier_linear.predict(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot up those grid with the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_classes = grid_classes.reshape(xx.shape)\n",
    "cmap = ListedColormap(['C2', 'C0', 'C1']) # Define the colors to make the decision boundary and the data points with the same colors\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "## Plot the decision boundary; Hint: use the plt.contourf() function\n",
    "\n",
    "## Add real data points\n",
    "\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the unknown points onto this classified grid and see what their assignment would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = ListedColormap(['C2', 'C0', 'C1'])\n",
    "grid_classes = grid_classes.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "## Plot the decision boundary; Hint: use the plt.contourf() function\n",
    "\n",
    "## Add real data points\n",
    "\n",
    "## Add the unknow points\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can visually see where the points fall, we can also ask the classifier to predict the values of these unknown points using `classifier_svc_linear.predict()`. \n",
    "\n",
    "We can return the actual labels of the data (rather than the encoded numbers) by using `le.inverse_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict the class of the unknown points\n",
    "prediction_point1_encoded = \n",
    "prediction_point2_encoded = \n",
    "prediction_point3_encoded = \n",
    "\n",
    "## Decode the predicted classes into the original labels; Hint: use the inverse_transform method of the LabelEncoder class\n",
    "prediction_point1 = \n",
    "prediction_point2 = \n",
    "prediction_point3 = \n",
    "\n",
    "## Print the results\n",
    "print(f\"Sample 1 {sample1} is classified as class {prediction_point1_encoded}, which is {prediction_point1}\")\n",
    "print(f\"Sample 2 {sample2} is classified as class {prediction_point2_encoded}, which is {prediction_point2}\")\n",
    "print(f\"Sample 3 {sample3} is classified as class {prediction_point3_encoded}, which is {prediction_point3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "\n",
    "How good is our linear classifier? To answer this we'll need to find out how frequently our classifications are correct.\n",
    "\n",
    "**Discussion question**\n",
    "\n",
    "*How should we determine the accuracy of this classification scheme using the data that we already have?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Do We Need to Set Training, Validation, and Test Datasets?\n",
    "\n",
    "When building a machine learning model, it's crucial to evaluate its performance accurately and ensure it generalizes well to new, unseen data. To achieve this, we typically split our dataset into three parts: training, validation, and test sets.\n",
    "\n",
    "- The training set is used to train the model. The model learns the patterns and relationships within this data. The goal is to minimize the error on the training set by adjusting the model's parameters.\n",
    "- The validation set is used to tune the model's hyperparameters. It helps in assessing the model's performance during the training phase and prevents overfitting. By evaluating the model on the validation set, we can choose the best model configuration.\n",
    "- The test set is used to evaluate the final model's performance. It provides an unbiased estimate of how well the model will perform on new, unseen data. The test set should only be used once the model is fully trained and all hyperparameters are tuned.\n",
    "\n",
    "### Variance and Bias Trade-Off\n",
    "\n",
    "In machine learning, the variance and bias trade-off is a fundamental concept that affects model performance.\n",
    "\n",
    "- Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).\n",
    "- Variance refers to the error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause the model to model the random noise in the training data rather than the intended outputs (overfitting).\n",
    "\n",
    "**Trade-Off**:\n",
    "- **High Bias, Low Variance:** The model is too simple and cannot capture the underlying patterns of the data (underfitting).\n",
    "- **Low Bias, High Variance:** The model is too complex and captures noise in the training data (overfitting).\n",
    "- **Optimal Balance:** The goal is to find a balance where the model has low bias and low variance, meaning it generalizes well to new data.\n",
    "\n",
    "By using training, validation, and test sets, we can monitor and adjust our model to achieve this balance, ensuring it performs well on unseen data. \n",
    "\n",
    "Let's try it in our classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing sets\n",
    "\n",
    "Because we are using a linear classifier, there are not many hyperparameters to tune. For simplicity, we will only divide the data into training and testing sets, using 70% of the data for training and 30% for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's split the data into training and testing sets using the train_test_split function from scikit-learn with a test size of 30%\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the number of samples in the training and testing sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the classifier using the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the testing data\n",
    "\n",
    "The test set was held back from training. We can use it to evaluate the model. How often are the categorizations correction? To do this evaluation, we can predict the categories using `classifier_svc_linear.predict()` and the compare those to the actual labels using `accuracy_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make predictions on the test set\n",
    "y_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the accuracy using the accuracy_score function\n",
    "accuracy = \n",
    "print(f\"The accuracy of the classifier is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above `accuracy_score` computes the proportion of correct predictions out of the total number of predictions. The accuracy score ranges from 0 to 1, where a higher score indicates better classification performance.\n",
    "\n",
    "We can make a plot that is the classification based on the training data and can then plot the test data. The fraction of points that are plotting within the correct classification corresponds to the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_classes = classifier_linear.predict(grid)\n",
    "grid_classes = grid_classes.reshape(xx.shape)\n",
    "\n",
    "cmap = ListedColormap(['C2', 'C0', 'C1'])\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolor='k', s=50)\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did the linear classifier do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the classification using a \"confusion matrix\"\n",
    "\n",
    "A confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It visually displays the accuracy of a classifier by comparing its predicted labels against the true labels. The matrix consists of rows and columns that represent the true and predicted classes, respectively. Each cell in the matrix corresponds to the number of samples for a specific combination of true and predicted class labels.\n",
    "\n",
    "The main diagonal of the matrix represents the correctly classified instances, while the off-diagonal elements represent the misclassified instances. By analyzing the confusion matrix, you can gain insights into the performance of the classifier and identify where it gets \"confused.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "## Calculate the confusion matrix using the confusion_matrix function\n",
    "cm =\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "class_names = le.inverse_transform(np.unique(y_test)).astype(str)\n",
    "\n",
    "## Plot the confusion matrix using ConfusionMatrixDisplay\n",
    "disp = \n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d', colorbar=False)\n",
    "\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you interpret the confusion matrix? What does it tell you about the performance of the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
