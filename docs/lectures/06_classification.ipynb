{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning: classification of basalt source\n",
    "\n",
    "## Import scientific python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying volcanic rocks\n",
    "\n",
    "<img src=\"./images/volcanic-tectonics.png\" width = 600 align = 'center'>\n",
    "\n",
    "Today we will continue to deal with igneous geochemistry data. Igneous rocks are those that crystallize from cooling magma. Different magmas have different compositions associated with their origin as we explored a week ago. \n",
    "During class today, we will focus on data from mafic lava flows (these are called basalts and are the relatively low silica, high iron end of what we looked at last week).\n",
    "\n",
    "> Igneous rocks form in a wide variety of tectonic settings,\n",
    "including mid-ocean ridges, ocean islands, and volcanic\n",
    "arcs. It is a problem of great interest to igneous petrologists\n",
    "to recover the original tectonic setting of mafic rocks of the\n",
    "past. When the geological setting alone cannot unambiguously\n",
    "resolve this question, the chemical composition of\n",
    "these rocks might contain the answer. The major, minor,\n",
    "and trace elemental composition of basalts shows large\n",
    "variations, for example as a function of formation depth\n",
    "(e.g., Kushiro and Kuno, 1963) --- *Vermeesch (2006)*\n",
    "\n",
    "For this analysis, we are going to use a dataset that was compiled in \n",
    "\n",
    "Vermeesch (2006) Tectonic discrimination of basalts with classification trees, *Geochimica et Cosmochimica Acta*  https://doi.org/10.1016/j.gca.2005.12.016\n",
    "\n",
    "These data were grouped into 3 categories:\n",
    "\n",
    "- 256 ***Island arc basalts (IAB)*** from the Aeolian, Izu-Bonin, Kermadec, Kurile, Lesser Antilles, Mariana, Scotia, and Tonga arcs.\n",
    "- 241 ***Mid-ocean ridge (MORB)*** samples from the East Pacific Rise, Mid Atlantic Ridge, Indian Ocean, and Juan de Fuca Ridge.\n",
    "- 259 ***Ocean-island (OIB)*** samples from St. Helena, the Canary, Cape Verde, Caroline, Crozet, Hawaii-Emperor, Juan Fernandez, Marquesas, Mascarene, Samoan, and Society islands.\n",
    "\n",
    "**Let's look at the illustration above and determine where each of these settings are within a plate tectonic context**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "\n",
    "The data are from the supplemental materials of the Vermeesch (2006) paper. The samples are grouped by affinity MORB, OIB, and IAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data = pd.read_csv('data/Vermeesch2006.csv')\n",
    "basalt_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basalt_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can geochemical data be used to classify the tectonic setting?\n",
    "\n",
    "These data are labeled. The author already determined what setting these basalts came from. However, is there are way that we could use these labeled data to determine the setting for an unknown basalt?\n",
    "\n",
    "A paper published in 1982 proposed that the elements titanium and vanadium were particular good at giving insight into tectonic setting. The details of why are quite complicated and can be summarized as \"the depletion of V relative to Ti is a function of the fO2 of the magma and its source, the degree of partial melting, and subsequent fractional crystallization.\" If you take EPS100B you will learn more about the fundamentals behind this igneous petrology. *For the moment, you can consider the working hypothesis behind this classification to that different magmatic environments have differences in oxidation states that are reflected in Ti vs V ratios.*\n",
    "\n",
    "Shervais, J.W. (1982) Ti-V plots and the petrogenesis of modern and ophiolitic lavas *Earth and Planetary Science Letters* https://doi.org/10.1016/0012-821X(82)90120-0\n",
    "\n",
    "### Plot TiO2 (wt%) vs V (ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for affinity in basalt_data['affinity'].unique():\n",
    "    subset = basalt_data[basalt_data['affinity'] == affinity]\n",
    "    plt.scatter(subset['TiO2_wt_percent'], subset['V_ppm'], label=affinity, edgecolor='k', s=50)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('TiO2 (wt%)')\n",
    "plt.ylabel('V (ppm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import more `sklearn` tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Linear logistic regression\n",
    "\n",
    "- Using only $TiO_2$ and V, we can use logistic regression to classify the data into MORB, OIB, and IAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data to define the features X and the target y\n",
    "basalt_data_Ti_V = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) & (~basalt_data['V_ppm'].isna())]\n",
    "X = basalt_data_Ti_V[['TiO2_wt_percent', 'V_ppm']].values\n",
    "y = basalt_data_Ti_V['affinity'].values\n",
    "\n",
    "## Encode the target variable from string to integer\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(solver=\"liblinear\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More classification methods\n",
    "\n",
    "Today we will explore a few more classification methods:\n",
    "\n",
    "- Support vector machines\n",
    "- Decision trees (Random forests)\n",
    "- Nearest neighbors\n",
    "- Neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a nice simplicity to the linear classifier approach comparing the $TiO_2$ vs V data, we have a lot more information from other aspects of the geochemistry. We might as well use that information as well.\n",
    "\n",
    "Let's use all the data we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data to use features other than TiO2 and V\n",
    "\n",
    "1. Encode the target variable 'affinity' using LabelEncoder: The target variable 'affinity' contains categorical data, which needs to be encoded as numerical values for the decision tree classifier. The LabelEncoder from scikit-learn is used to transform the 'affinity' column into numerical labels.\n",
    "\n",
    "2. Split the data into features (X) and target (y): The dataset is split into two parts, features (X) and the target variable (y). Features are the input variables that the classifier will use to make predictions, and the target variable is the output we want the classifier to predict.\n",
    "\n",
    "3. Impute missing values using median imputation: most classifiers cannot handle missing values in the input data. Therefore, missing values in the dataset need to be imputed (filled in) before training the classifier. Let's use median imputation which replaces the missing values with the median of the non-missing values in the same column. We can import and use the `SimpleImputer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data into features (X) and target (y)\n",
    "X = basalt_data.drop('affinity', axis=1)\n",
    "y = basalt_data['affinity']\n",
    "\n",
    "## We will keep the column names for later use\n",
    "columns = X.columns\n",
    "\n",
    "## Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "## Impute missing values using median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "## Split the data into training and test sets using 30% of the data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a support vector machine?\n",
    "\n",
    "Support vector machines are a type of supervised learning model that can be used for both classification and regression. The basic idea is to find the hyperplane that best separates the classes in the feature space. The hyperplane is defined by the support vectors, which are the points that are closest to the hyperplane. The distance between the support vectors and the hyperplane is called the margin. The goal of the SVM is to maximize the margin, which is done by minimizing the norm of the weight vector. The SVM can be used for both linear and non-linear classification by using different kernel functions.\n",
    "\n",
    "## How does the SVM work?\n",
    "\n",
    "Support vector machines work by finding the hyperplane that best separates the classes in the feature space. The hyperplane is defined by the support vectors, which are the points that are closest to the hyperplane. The distance between the support vectors and the hyperplane is called the margin. The goal of the SVM is to maximize the margin, which is done by minimizing the norm of the weight vector. The SVM can be used for both linear and non-linear classification by using different kernel functions.\n",
    "\n",
    "![](images/SVM-basic.png)\n",
    "\n",
    "## What is a kernel function?\n",
    "\n",
    "A kernel function is a function that takes two input vectors and returns a scalar value. The kernel function is used to map the input vectors into a higher-dimensional space where the classes are linearly separable. The most common kernel functions are the linear kernel, the polynomial kernel, and the radial basis function (RBF) kernel.\n",
    "\n",
    "![](images/SVM-kernel.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a support vector machine classifier for our data\n",
    "\n",
    "First, we will use a linear kernel to classify the data. Same as before, we will use the `SVC` class from the `sklearn.svm` module. We will use the `fit` method to train the model on the training data and the `predict` method to make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Define the classifier using make_pipeline\n",
    "classifier = make_pipeline(StandardScaler(), SVC(kernel=\"linear\"))\n",
    "\n",
    "## Train the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to classify the data using a non-linear kernel. We will use the radial basis function (RBF) kernel, which is the default kernel for the `SVC` class. The RBF kernel is defined as:\n",
    "\n",
    "$$K(x, x') = \\exp(-\\gamma ||x - x'||^2)$$\n",
    "\n",
    "where $\\gamma$ is a hyperparameter that controls the smoothness of the decision boundary. A larger value of $\\gamma$ will result in a more complex decision boundary, which can lead to overfitting. We will use the default value of $\\gamma$ for now, but we can tune it later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Define the classifier using make_pipeline and the RBF kernel\n",
    "gamma = 0.03 ## gamma is a hyperparameter of the RBF kernel\n",
    "classifier = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", gamma=gamma))\n",
    "\n",
    "## Train the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "## Predict the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "## Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "## Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d', colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a **Decision Trees** approach which is another supervised machine learning algorithm for classification \n",
    "\n",
    "**Decision Trees** are a type of flowchart-like structure where internal nodes represent decisions based on the input features, branches represent the outcome of these decisions, and leaf nodes represent the final output or class label. The primary goal of a decision tree is to recursively split the data into subsets based on feature values that maximize the separation between the classes.\n",
    "\n",
    "*Why use Decision Trees?*\n",
    "\n",
    "- Easy to understand and interpret: Decision Trees are human-readable and can be visualized, making them easy to understand and interpret even for those with limited machine learning experience.\n",
    "- Minimal data preprocessing: Decision Trees do not require extensive data preprocessing, such as scaling or normalization, as they can handle both numerical and categorical features.\n",
    "- Non-linear relationships: Decision Trees can model complex, non-linear relationships between features and target variables.\n",
    "- Feature importance: Decision Trees can provide insights into feature importance, helping to identify the most relevant features for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the `DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## Define the decision tree classifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "## Train the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "## Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "## Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 20))\n",
    "\n",
    "## Get the original class names\n",
    "class_names = le.inverse_transform(np.unique(y)).astype(str)\n",
    "\n",
    "## Visualize the decision tree\n",
    "plot_tree(classifier, filled=True, feature_names=columns, class_names=class_names, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What aspects of the data are important for the classification?\n",
    "\n",
    "We want to be able to readily determine what data fields are the most important for the decision tree. We can do that by determining \"feature importance.\" The code below extracts and displays the importance score, also known as Gini importance or Mean Decrease Impurity, which is a measure of how much a feature contributes to the decision-making process of the decision tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the feature importances from the classifier\n",
    "importances = classifier.feature_importances_\n",
    "\n",
    "## Pair the feature names with their corresponding importances\n",
    "feature_importances = list(zip(columns, importances))\n",
    "\n",
    "## Create a DataFrame from the feature importances\n",
    "df_feature_importances = pd.DataFrame(feature_importances, columns=['Feature', 'Importance'])\n",
    "\n",
    "## Sort the feature importances in descending order\n",
    "df_feature_importances = df_feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "## Reset the index and drop the old index column\n",
    "df_feature_importances.reset_index(drop=True, inplace=True)\n",
    "\n",
    "## Display the sorted feature importances\n",
    "display(df_feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "*If we were going to build a classifier on just two variables, which should be pick?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring other classification algorithms\n",
    "\n",
    "If you go to the scikit-learn homepage you will find many available classifiers: https://scikit-learn.org/stable/index.html. They are nicely illustrated in this code from the scikit-learn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data\n",
    "\n",
    "The decision tree suggested that Ti and Sr were the biggest differentiators. Let's have a look at Ti (wt %) vs Sr (ppm) and apply different classifying algorithms. For many of these algorithms, it is essential to normalize the data. For example, the nearest neighbor is a distance. Consider that in the TiO2 (wt%) vs. Sr (ppm) or V (ppm) the y-axis and x-axis are so different (in part because of different units). So we need to normalize the data.\n",
    "\n",
    "We can use the `sklearn.preprocessing` function `StandardScaler` to help us here.\n",
    "\n",
    "Let's make a `basalt_data_Ti_Sr` dataframe and then apply the `StandardScaler` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "basalt_data_Ti_Sr = basalt_data[(~basalt_data['TiO2_wt_percent'].isna()) & (~basalt_data['Sr_ppm'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract the necessary features and target variable\n",
    "X = basalt_data_Ti_Sr[['TiO2_wt_percent', 'Sr_ppm']].values\n",
    "y = basalt_data_Ti_Sr['affinity'].values\n",
    "\n",
    "# Encode the target variable 'affinity' using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted scaler\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply different classifiers\n",
    "\n",
    "Now that we have normalized the data, we can apply classifiers, I have put in the `KNeighborsClassifier(3)`. **What are the strengths and weaknesses of this classifier?**\n",
    "\n",
    "Play around with switching the classifier to other ones from the example above. **What would the best classifier to use?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## Define the KNN classifier with 3 neighbors using make_pipeline\n",
    "classifier = make_pipeline(StandardScaler(), KNeighborsClassifier(3))\n",
    "\n",
    "## Train the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "## Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "## Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the decision boundary for the KNN classifier. The decision boundary is the line that separates the different classes in the feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid \n",
    "x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 20), np.linspace(y_min, y_max, 20))\n",
    "\n",
    "# Classify the grid points using the classifier\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_classes = classifier.predict(grid)\n",
    "\n",
    "# Reshape the predicted class labels to match the shape of the input grid\n",
    "grid_classes = grid_classes.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary and the test data points\n",
    "cmap = ListedColormap(['C2', 'C0', 'C1'])\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, grid_classes, cmap=cmap, alpha=0.6)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap, edgecolors='k', marker='o', s=50)\n",
    "\n",
    "# Add a legend\n",
    "cbar = plt.colorbar(ticks=[0.375, 1., 1.625])\n",
    "cbar.set_ticklabels(le.inverse_transform([0, 1, 2]))\n",
    "\n",
    "plt.xlabel('Normalized TiO2_wt_percent')\n",
    "plt.ylabel('Normalized Sr_ppm')\n",
    "plt.title('Classifier (Test Data with Decision Boundary)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word of warning\n",
    "\n",
    "As a word of warning, we shouldn't get too carried away. Clearly, there are complexities related to this approach (our accuracy scores aren't that high). There are other types of contextual data that can give insight. For example, Shervais (1982) notes that: \n",
    "> \"More specific evaluation of the tectonic setting of these and other ophiolites requires\n",
    "application of detailed geologic and petrologic data as well as geochemistry. The Ti/V discrimination diagram, however,\n",
    "is a potentially powerful adjunct to these techniques.\"\n",
    "\n",
    "Vermeesch adds to this point by writing:\n",
    "> \"no classification method based solely on geochemical\n",
    "data will ever be able to perfectly determine the\n",
    "tectonic affinity of basaltic rocks (or other rocks for that\n",
    "matter) simply because there is a lot of actual overlap between\n",
    "the geochemistry of the different tectonic settings.\n",
    "Notably IABs have a much wider range of compositions\n",
    "than either MORBs or OIBs. Therefore, geochemical classification\n",
    "should never be the only basis for determining\n",
    "tectonic affinity. This is especially the case for rocks that\n",
    "have undergone alteration. In such cases, mobile elements\n",
    "such as Sr, which have great discriminative power, cannot\n",
    "be used.\"\n",
    "\n",
    "Additionally, we would like to be able to assign physical processes to any classification given that we are seeking insight into how the Earth works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
