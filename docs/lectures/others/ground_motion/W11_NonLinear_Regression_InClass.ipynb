{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    " <!-- BEGIN QUESTION -->\n",
    "# Non-Linear Regression In Class Exercise\n",
    "\n",
    "**Our goals for today:**\n",
    "- Load peak ground acceleration observations from two notable M6 quakes in California\n",
    "- Attempt to fit data using `polyfit()`\n",
    "- Develop a physics-based model and fit to data\n",
    "- Vary assumed mean event depth to find better fitting model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell as it is to setup your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import otter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "# Analysis of Strong Ground Motion Data\n",
    "\n",
    "Earthquakes are the sudden dislocation of rock on opposite sides of a fault due to applied stress. Seismic waves are generated by this process and propagate away from the fault affecting nearby communities. It is the strong shaking from earthquakes that we recognize as the earthquake. These motions can lead to landslides, liquefaction of the ground, and of course impact anything built within or on the ground. The motions generated by fault dislocation affect many aspects of modern society. Earthquake Engineering is a field that studies the ground motions generated by earthquakes and how they affect the built environment. To utilize ground motions for engineering applications requires studying the physics of seismic wave propagation, and the development of models that effectively describe it. Of particular importance is the need to accurately model and predict seismic wave amplitudes. Such studies generally focus on examining the peak acceleration and velocity as a function of distance from the source. The physics indicates that the ground motions generally decrease in amplitude with increasing distance.\n",
    "\n",
    "On August 24, 2014 a M6 earthquake occurred in south Napa. The following figure shows the observed strong ground acceleration. There is a lot of complexity in the distribution that seismologists and earthquake engineers need to consider, but the general trend is that the ground motions decrease with distance from the earthquake.\n",
    "\n",
    "<img src=\"./napa_obsgm.png\" width=500>\n",
    "\n",
    "In this module we will combine acceleration ground motion observations from two M6 events (2014 Napa, and 2004 Parkfield) to have a more complete distance distribution of observations. We will analyze the data first by attempting to fit curves as we have done for other datasets in the class (sea floor age, sea floor magnetism, distance and velocity of supernovae). We will then examine a physics-based model and a variety of methods to fit data. A model that describes the decrease (attenuation) of strong ground motion data over the years has been called 'attenuation relationships', 'ground motion prediction equations (GMPE)' and most recently 'ground motion models (GMM)'. Whatever it is called it is a fundamental to being able to characterized strong ground motion of future earthquakes and is used by the USGS and collaborators to develop earthquake forecast maps. GMM information coupled with the statistics of earthquake occurrence rates, notably Gutenberg-Richter statistics, provides the frame work for characterizing future ground motion hazard, as illustrated in the following map (red is high shaking hazard).\n",
    "\n",
    "<img src=\"./2018nshm-longterm.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Part 1, Load, Plot and Fit Models to Peak Ground Acceleration Data\n",
    "\n",
    "We will make use of peak ground acceleration data from the 2014 Napa and 2004 Parkfield earthquakes. The acceleration is given in units of 'g', where 1g is 981 $\\frac{cm}{s^2}$. Earthquake Engineers commonly use the peak ground acceleration in such units in their geotechnical materials and structural engineering analyses. 0.1%g is the level people generally can perceive shaking, at 2%g some people may be disoriented, at 50% the shaking is very violent and unengineered structures can suffer damage and collapse, while well engineered buildings can survive if the duration is short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Read the Peak Ground Acceleration data\n",
    "park_pga=pd.read_csv('parkfieldeq_pga.csv')\n",
    "napa_pga=pd.read_csv('napaeq_pga.csv')\n",
    "park_pga.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Plot the two data sets\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(park_pga['Dist(km)'],park_pga['PGA(g)'],'.',color='blue',alpha=0.2)\n",
    "plt.plot(napa_pga['Dist(km)'],napa_pga['PGA(g)'],'.',color='green')\n",
    "ax.set(xlabel='Distance (km)', ylabel='Peak ground acceleration (g)',\n",
    "       title='Peak Acceleration Data Linear Plot')\n",
    "plt.legend(['Napa','Parkfield'],fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.loglog(park_pga['Dist(km)'],park_pga['PGA(g)'],'.',color='blue',alpha=0.2)\n",
    "plt.loglog(napa_pga['Dist(km)'],napa_pga['PGA(g)'],'.',color='green')\n",
    "ax.set(xlabel='Distance (km)', ylabel='Peak ground acceleration (g)',\n",
    "       title='Peak Acceleration Data Log Plot')\n",
    "plt.legend(['Napa','Parkfield'],fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Combine the two similar magnitude earthquake data\n",
    "dist=np.concatenate((np.array(napa_pga['Dist(km)']),np.array(park_pga['Dist(km)'])))\n",
    "pga=np.concatenate((np.array(napa_pga['PGA(g)']),np.array(park_pga['PGA(g)'])))\n",
    "\n",
    "#Examine individual earthquake data\n",
    "#dist=np.array(park['Dist(km)'])\n",
    "#pga=np.array(park['PGA(g)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "#### First. try fitting the data with standard curves as we did before using np.polyfit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Try fitting data with np.polyfit()\n",
    "p=np.polyfit(...)\n",
    "x=np.arange(0.1,np.max(dist),0.1)\n",
    "y=np.polyval(p,x)\n",
    "\n",
    "plt.plot(dist,pga,'.',color='blue')\n",
    "plt.plot(x,y,'-',color='red')\n",
    "plt.xlabel('Distance(km)')\n",
    "plt.ylabel('Peak Ground Acceleration (g)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "#### How well can the data be fit with polynomials?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "#### Try fitting the data with a power law ($pga = \\frac{a}{dist^b}$)\n",
    "\n",
    "- To do this we linearize the equation to use polyfit() for a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#dist=dist+1    #add a small number to avoid singularity (dist=0)\n",
    "p=np.polyfit(...)\n",
    "print(p)\n",
    "x=np.arange(np.min(dist),np.max(dist),0.1)\n",
    "y=np.polyval(p,np.log(x))\n",
    "\n",
    "#dist=dist-1\n",
    "plt.plot(dist,pga,'.',color='blue')\n",
    "plt.plot(x,np.exp(y),'-',color='red')\n",
    "plt.xlabel('Distance(km)')\n",
    "plt.ylabel('Peak Ground Acceleration (g)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "#### How well does a power law fit?\n",
    "\n",
    "What is wrong with this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Part 2, Fitting Strong Motion Data\n",
    "\n",
    "In order to use the observations of peak ground acceleration to characterize seismic ground motion hazard it is necessary to develop a model that accurately describes the behavior seismic wave propagation, for example how the waves travel through the earth and dissipate. From physics seismic ground motions decay as a power law with distance (referred to as geometrical spreading), but we saw earlier that a power law alone does not work well, it is linear in log-space, where it does not explain the plateauing of ground motions close to the earthquake.\n",
    "\n",
    "To fix this we also need to consider that waves travel upward as well as away from an earthquake where\n",
    "$r=\\sqrt{(dist^2 + h^2)}$ is the total distance comprised of the horizontal distance and the depth (h) of the earthquake.\n",
    "\n",
    "Finally, in addition to geometrical spreading, there is an inelastic attenuation term that accounts for dissipative energy loss due to material imperfections. Based on this theory the following is a simple relationship that describes the dissipation or attenuation of seismic wave energy with distance from the earthquake, \n",
    "\n",
    "$pga=a*{\\frac{1}{r^b}}*e^{cr}$,\n",
    "\n",
    "where $a$ is a coeffient that depends on magnitude and scales the overall motions, $b$ is the exponent for the power-law geometrical spreading term, and $c$ is the coefficient for the in-elastic term (important only at large distances), and r is the total distance that considers the depth of the earthquake (h). Note that in the far-field the theoretical geometrical spreading decay of ground motions is ~1/r (in the near-field it is ~$1/r^2$). This is a non-linear equation, but it can be linearized by taking the natural logarithm.\n",
    "\n",
    "$\\mathrm{ln}(pga)=a + b*\\mathrm{ln}(r) + c*r$\n",
    "\n",
    "- How do we setup this inverse problem? Let's first consider a simple linear example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### How to setup a linear (linearized) inverse problem\n",
    "\n",
    "- Until now we have been using \"canned\" functions to fit lines, or polynomials to data, but this doesn't always work because 1) sometimes more complicated functions are needed, 2) functions are non-linear, 3) we need to fit a physics-based model to the data.\n",
    "\n",
    "- We can construct our own inverse problem to fit more complex functions, as illustrated below.\n",
    "\n",
    "- When fitting a model such as a line to data, each data point can be considered a separate equation of two variables (a, b). That is for each x value there is a corresponding y value related to x through the equation for a line, where a is the intercept and b is the slope of the line. \n",
    "\n",
    "<img style=\"right;\" src=\"./linear_eq_cartoon.png\" width=\"500\">\n",
    "\n",
    "- The system of equations can be constructed in matrix form, and least squares (or other methods may be used to solve the matrix equation for the model parameters. Some of the functions we have been using are doing this \"under the hood\".\n",
    "\n",
    "#### Let's try it for a simple linear case\n",
    "\n",
    "1. Consider data from a line with some random noise added\n",
    "2. Fit data using polyfit()\n",
    "3. Construct the linear inverse problem from basic principles\n",
    "4. Apply non-linear least-squares scipy.optimize.curve_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "x=np.array((1, 2.2, 4.3, 7.7))\n",
    "data=-1.5 + 3*x                   #construct data with an intercept of -1.5 and slope of 3.\n",
    "\n",
    "#random number array\n",
    "#rand=np.random.uniform(low=-2., high=2.0, size=4)    #apply random numbers\n",
    "#data=data + rand\n",
    "\n",
    "m=np.polyfit(x,data,1)\n",
    "\n",
    "plt.plot(x,data,'o',color='blue')\n",
    "#syn=np.polyval(m,x)\n",
    "#plt.plot(x,syn,'-',color='red')\n",
    "plt.show()\n",
    "\n",
    "print(f'From polyfit(): a={m[1]:.2f}  b={m[0]:.2f}')\n",
    "\n",
    "#Solve via least squares\n",
    "A=np.vstack((...,...)).transpose()\n",
    "#AtA=np.dot(...)\n",
    "#AtD=np.dot(...)\n",
    "#a, b=np.linalg.solve(...)\n",
    "#print(f'From manual least squares: a={a:.2f}  b={b:.2f}')\n",
    "\n",
    "#Now lets use the scipy non-linear least-squares curve_fit() method\n",
    "#def linmod(x,a,b):\n",
    "#    return ...\n",
    "\n",
    "#m=curve_fit(linmod,x,data)[0]\n",
    "#print(f'From curve_fit(): a={m[0]:.2f}  b={m[1]:.2f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### Now Setup a lineared inverse problem for the PGA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Setup a linearized inverse problem for Parkfield\n",
    "h=4.0  #Assume a depth (km)\n",
    "r=np.sqrt(dist**2 + h**2)\n",
    "\n",
    "#Setup G matrix\n",
    "intercept_term=\n",
    "ln_term=\n",
    "exp_term=\n",
    "G=\n",
    "\n",
    "#Setup Data Matrix\n",
    "d=\n",
    "\n",
    "#Setup of least squares\n",
    "gtg=np.dot(...)\n",
    "gtd=np.dot(...)\n",
    "\n",
    "#Solve for a, b, c\n",
    "a, b, c=np.linalg.solve(gtg,gtd)\n",
    "\n",
    "#Measure fit\n",
    "m=np.array((a,b,c))\n",
    "syn=np.exp(a + b*np.log(r) + c*r)\n",
    "rms_fit=np.sqrt(np.mean((pga - syn)**2))\n",
    "print(f'(a,b,c)={a:.3f}/{b:.3f}/{c:.3f}  RMS={rms_fit:.3f}')\n",
    "\n",
    "#Plot results\n",
    "x=np.arange(0.0,np.max(dist),0.1)\n",
    "xr=np.sqrt(x**2 + h**2)\n",
    "y=np.exp(a + b*np.log(xr) + c*xr)\n",
    "plt.loglog(dist,pga,'.',color='blue')\n",
    "plt.loglog(x,y,'-',color='red')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "#### How well does this inversion perform? Are the model parameters consistent with the theory for geometrical spreading and anelastic attenuation?\n",
    "\n",
    "- write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Part 3, Apply Non-linear least-squares\n",
    "\n",
    "The model that we are trying to fit is non-linear in distance so it makes sense to try the non-linear least-squares method. We will also discover that with this optimization method we can find solution with a assumed range of parameters that can be constraint by our understanding of the physics or by some other observations.\n",
    "\n",
    "Non-linear optimization is a topic that requires an entire semester by itself, and would include non-linear least-squares, grid-search (though slow for large data sets), Montecarlo sampling, Bayesian inference, genetic algorithm, etc.\n",
    "\n",
    "We will use the scipy.optimization.curve_fit() which utilizes non-linear least squares. So that this is not entirely a black box, briefly non-linear least-squares involves using a starting model to estimate a prediction error, differentiating the prediction error with respect to model parameters, and then updating the model and repeating until convergence is achieved. This wiki describes it in some detail. https://en.wikipedia.org/wiki/Non-linear_least_squares\n",
    "\n",
    "If $y$ is the data and $f(x, m)$ is the prediction as a function of (m) model parameters then the initial prediction error is $e_i=(y_i - f(x_i, m_0))$. Given an initial model $m_0$, $f$ can be represented as a Taylor series where $f(x_i, m_1)=f(x_i, m_0) + \\frac{\\partial f}{\\partial m}(m_1 - m_0)$=$f(x_i, $m_0$) + \\frac{\\partial f}{\\partial m}(\\Delta m)$=$y_i$. Combining the prediction error and Taylor series equations gives:\n",
    "\n",
    "$e_i=[\\frac{\\partial f}{\\partial m}](\\Delta m)$, which as the form of the previous matrix equation we used. Suppose m=(a,b), and f(m)=a+bx then this results in a system of equations:\n",
    "\n",
    "$e_1=\\frac{\\partial f}{\\partial a}\\rvert_{x_1}\\Delta a + \\frac{\\partial f}{\\partial b}\\rvert_{x_1}\\Delta b$\n",
    "\n",
    "$e_2=\\frac{\\partial f}{\\partial a}\\rvert_{x_2}\\Delta a + \\frac{\\partial f}{\\partial b}\\rvert_{x_2}\\Delta b$\n",
    "\n",
    "$e_N=\\frac{\\partial f}{\\partial a}\\rvert_{x_N}\\Delta a + \\frac{\\partial f}{\\partial b}\\rvert_{x_N}\\Delta b$\n",
    "\n",
    "If $m_0$=(0,0) then the system of equations becomes what we found for the linear least-squares problem, where:\n",
    "\n",
    "$y_1=a + bx_1$\n",
    "\n",
    "$y_2=a + bx_2$\n",
    "\n",
    "$y_N=a + bx_N$\n",
    "\n",
    "The following is the general non-linear least-squares equation:\n",
    "\\begin{equation*}\n",
    "Y=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial m_1}\\rvert_{x_1} & \\frac{\\partial f}{\\partial m_1}\\rvert_{x_1} & \\cdots & \\frac{\\partial f}{\\partial m_M}\\rvert_{x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial m_1}\\rvert_{x_2} & \\frac{\\partial f}{\\partial m_1}\\rvert_{x_2} & \\cdots &\\frac{\\partial f}{\\partial m_M}\\rvert_{x_2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial m_1}\\rvert_{x_N} & \\frac{\\partial f}{\\partial m_1}\\rvert_{x_N} & \\cdots & \\frac{\\partial f}{\\partial m_M}\\rvert_{x_N}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\begin{bmatrix}\n",
    "\\Delta m_1 \\\\\n",
    "\\Delta m_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\Delta m_M\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Test the scipy curve_fit method\n",
    "\n",
    "#Define the non-linear function\n",
    "def gm_model(x,a,b,c):\n",
    "    #This function returns ln(pga)\n",
    "    return ...\n",
    "\n",
    "h=4.0\n",
    "r=np.sqrt(dist**2 + h**2)\n",
    "m=curve_fit(...,..., ...,bounds=([...,...,...],...))[0]\n",
    "\n",
    "#Measure fit\n",
    "syn=np.exp(gm_model(r,m[0],m[1],m[2]))\n",
    "rms_fit=np.sqrt(np.mean((pga - syn)**2))\n",
    "print(f'(a,b,c,h)={m[0]:.3f}/{m[1]:.3f}/{m[2]:.3f}  RMS={rms_fit:.3f}')\n",
    "\n",
    "plt.loglog(dist,pga,'.')\n",
    "x=np.arange(0.1,200,0.1)\n",
    "xr=np.sqrt(x**2 + h**2)\n",
    "y=np.exp(gm_model(xr,m[0],m[1],m[2]))\n",
    "plt.loglog(x,y,'-',color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### Compute 95% confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Compute 95% confidence levels\n",
    "degfree=len(r)-3                           #degrees of freedom (num data - num model params)\n",
    "e=np.log(pga)-np.log(syn)                  #residuals between data and model\n",
    "var=np.sum(e**2)/degfree                   #variance\n",
    "se_y=np.sqrt(var)                          #standard error of the estimate\n",
    "sdev=np.sqrt(var)                          #standard deviation\n",
    "#Calculate 95% confidence bounds\n",
    "t=stats.t.ppf(1-0.05/2,degfree)             #division by 2 to map from single-tail to dual-tail t-distribution\n",
    "lower95=np.exp(np.log(y)-t*se_y)\n",
    "upper95=np.exp(np.log(y)+t*se_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Plot Results\n",
    "fig, ax = plt.subplots()\n",
    "ax.loglog(dist,pga,'b.',x,y,'k-',linewidth=2)\n",
    "ax.loglog(x,lower95,'r-',x,upper95,'r-',linewidth=1)\n",
    "ax.set(xlabel='Distance (km)', ylabel='Peak ground acceleration (g)',\n",
    "       title='Peak Acceleration Data and Weighted Least Squares Inversion')\n",
    "#plt.legend(['Napa','Parkfield'],fontsize=12,loc=3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "#### Test our assumption that the mean depth of the earthquakes is 4.0km.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "What depth produces the best fitting model (minimum variance)? How sensitive is the model to depth? Consider depths ranging from say 1 to 20 km."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "#### Compare solutions using the Napa and Parkfield data separately and discuss how the results compare.\n",
    "\n",
    "Write you answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### Turn in this Notebook\n",
    "\n",
    "**Run the cell below to export this notebook as a pdf and upload to bCourses. Make sure to run all cells and save before doing so for changes to be reflected in the pdf.**\n",
    " <!-- END QUESTION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "grader = otter.Notebook()\n",
    "from otter.export import export_notebook\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"W09_NonLinear_Regression_InClass.ipynb\", filtering=True, pagebreaks=False)\n",
    "display(HTML(\"<p style='font-size:20px'> <br>Save this notebook, then click <a href='W09_NonLinear_Regression_InClass.pdf' download>here</a> to open the pdf.<br></p>\"))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
